# Rule Assistant Decision Log

**Version:** 1.0
**Date:** 2025-11-22
**Status:** Active
**Maintainer:** FLExTrans Development Team

---

## Purpose

This document records all significant architectural and design decisions made for the Rule Assistant enhancement project. Each decision includes:
- The decision made
- Context and problem being solved
- Alternatives considered
- Rationale for the chosen approach
- Trade-offs and implications
- Status and review history

---

## Table of Contents

1. [Decision Format](#decision-format)
2. [Active Decisions](#active-decisions)
3. [Superseded Decisions](#superseded-decisions)
4. [Deferred Decisions](#deferred-decisions)

---

## Decision Format

Each decision follows this template:

```
### DEC-NNN: Decision Title

**Status:** [Proposed | Accepted | Implemented | Superseded | Rejected]
**Date:** YYYY-MM-DD
**Deciders:** [Names/Roles]
**Priority:** [Critical | High | Medium | Low]
**Related Issues:** #NNN, FR-NNN

#### Context and Problem

[What problem are we trying to solve?]

#### Decision

[What did we decide to do?]

#### Alternatives Considered

1. **Alternative A**
   - Description
   - Pros
   - Cons

2. **Alternative B**
   - Description
   - Pros
   - Cons

#### Rationale

[Why did we choose this approach?]

#### Trade-offs and Implications

**Benefits:**
- ...

**Costs:**
- ...

**Risks:**
- ...

#### Implementation Notes

[Key implementation details, dependencies]

#### Review History

- YYYY-MM-DD: Decision proposed
- YYYY-MM-DD: Decision accepted
- YYYY-MM-DD: Implementation started
```

---

## Active Decisions

### DEC-001: Section Preservation Strategy

**Status:** Accepted
**Date:** 2025-11-22
**Deciders:** Development Team, User Feedback
**Priority:** Critical
**Related Issues:** FR-001

#### Context and Problem

Users reported that after running the Rule Assistant, `<section-def-vars>` and `<section-def-macros>` sections were being deleted from their transfer files, resulting in loss of manually-created content. This makes it impossible to safely combine Rule Assistant-generated rules with manual editing.

**Root cause:** `WriteTransferFile()` removes any section that is empty (`len(section) == 0`), without distinguishing between:
- Sections that are genuinely unused
- Sections that are empty now but should be preserved for future manual edits
- Sections that exist in the template and should remain

#### Decision

**Preserve ALL existing sections in the transfer file, regardless of whether they appear empty from the Rule Assistant's perspective.**

Specifically:
1. When loading an existing transfer file, mark all existing sections for preservation
2. Never delete sections, even if they contain no elements generated by RA
3. Only delete individual elements within sections if they are truly redundant (e.g., duplicate macro definitions)

#### Alternatives Considered

##### Alternative A: Selective Preservation Based on Metadata

**Description:** Add metadata to sections indicating whether they should be preserved
```xml
<section-def-vars preserve="yes">
  <!-- Will be preserved even if empty -->
</section-def-vars>
```

**Pros:**
- User has explicit control
- Clear intent documented in XML

**Cons:**
- Requires user to manually add preserve attribute
- Non-standard XML (not in Apertium DTD)
- Users won't know to add it until they lose data
- Breaks compatibility with existing files

**Verdict:** Rejected - Too much user burden, doesn't solve problem for existing files

##### Alternative B: Never Delete Any Section

**Description:** Preserve all standard Apertium sections, never delete any

**Pros:**
- Simple, clear policy
- No data loss ever
- Easy to implement

**Cons:**
- May leave unused sections in file
- File might accumulate cruft over time
- Could lead to larger files

**Verdict:** Accepted - Benefits far outweigh costs

##### Alternative C: Smart Detection of "User-Created" Content

**Description:** Try to detect whether sections contain user-created vs. RA-generated content
```python
if section.has_user_content():
    preserve_section()
else:
    delete_section()
```

**Pros:**
- Could preserve only what's needed
- Keeps files clean

**Cons:**
- Detection logic is complex and error-prone
- False positives = data loss
- Hard to determine origin of content
- Comments, formatting might indicate user edits
- Risk not worth the benefit

**Verdict:** Rejected - Too risky, too complex

#### Rationale

**Why we chose "preserve all sections":**

1. **Data loss is unacceptable:** Losing user work is the worst possible outcome. A slightly larger file is a minor inconvenience.

2. **Simple and reliable:** No complex detection logic, no edge cases, no false positives.

3. **User expectations:** Users expect their manual edits to be preserved. This matches that mental model.

4. **Apertium tolerates empty sections:** Empty sections in Apertium transfer files are valid and don't affect functionality.

5. **Supports mixed workflow:** Enables the recommended workflow of using both RA and manual editing together.

#### Trade-offs and Implications

**Benefits:**
- ✓ No data loss
- ✓ Users can safely combine RA + manual editing
- ✓ Simple implementation
- ✓ Easy to understand and document
- ✓ Matches user expectations

**Costs:**
- ✗ Transfer files may contain empty sections
- ✗ Files slightly larger
- ✗ Users might not realize a section is unused

**Risks:**
- Low risk: Empty sections don't affect Apertium functionality
- Mitigated: Users can manually delete unused sections if desired

**Implementation Impact:**
- Code change in `WriteTransferFile()`: Remove section deletion logic
- Code change in `ProcessExistingTransferFile()`: Add section preservation tracking
- Test coverage: Add tests for section preservation
- Documentation: Update user guide to explain what is preserved

#### Implementation Notes

**Key changes:**

```python
class RuleGenerator:
    def __init__(self, ...):
        self.preserveSections = set()  # Track sections to preserve

    def ProcessExistingTransferFile(self, fileName: str):
        """Load existing file and mark all sections for preservation."""
        tree = ET.parse(fileName)
        self.root = tree.getroot()

        # Mark ALL sections for preservation
        for section in self.root:
            self.preserveSections.add(section.tag)
            self.report.Info(f"Preserving section: {section.tag}")

    def WriteTransferFile(self, fileName: str):
        """Write file WITHOUT deleting preserved sections."""
        # OLD CODE (removed):
        # for section in self.root.findall('.//*[@n]'):
        #     if len(section) == 0:
        #         self.root.remove(section)  # DELETE

        # NEW CODE: Just write the file as-is
        tree = ET.ElementTree(self.root)
        tree.write(fileName, encoding='UTF-8', xml_declaration=True)
```

**Testing:**
- Test with empty sections (should be preserved)
- Test with manually-created variables (should be preserved)
- Test with manually-created macros (should be preserved)
- Regression test: Ensure existing functionality still works

#### Review History

- 2025-11-22: Decision proposed based on FR-001 analysis
- 2025-11-22: Decision accepted
- Status: Ready for implementation in Phase 1

---

### DEC-002: Macro Reuse Strategy (Issue #661)

**Status:** Accepted
**Date:** 2025-11-22
**Deciders:** Development Team
**Priority:** High
**Related Issues:** #661

#### Context and Problem

Multi-source macro reuse functionality was disabled due to problems:
1. Variable name conflicts
2. Incomplete macro matching (simple lookup key insufficient)
3. Macro content drift (hand-edited macros incorrectly reused)
4. Test failures

Current impact:
- 30-50% code duplication in transfer files
- Larger files, slower processing
- More memory usage

**Original lookup key was too simple:**
```python
lookupKey = (destCategory, tuple(sourceCats))
# PROBLEM: Same categories, different features collide
```

**Example collision:**
```python
# Macro 1: adj lemma from noun gender + verb tense
# Lookup key: ('adj', ('n', 'v'))

# Macro 2: adj lemma from noun number + verb aspect
# Lookup key: ('adj', ('n', 'v'))  # SAME KEY, DIFFERENT LOGIC!
```

#### Decision

**Use macro structure fingerprinting for deduplication.**

Generate an MD5 hash of the normalized macro structure to detect truly identical macros, regardless of variable/macro naming.

#### Alternatives Considered

##### Alternative A: Enhanced Lookup Key with Feature Signatures

**Description:** Include feature information in the lookup key
```python
def GetMacroLookupKey(destCategory, isLemma, sources):
    """Generate comprehensive lookup key including features."""
    catSequence = sorted(set([s.category for s in sources]))

    # Create feature signature
    featuresByCategory = defaultdict(list)
    for spec in sources:
        featuresByCategory[spec.category].append(
            (spec.label, spec.isAffix, spec.default, spec.ranking)
        )

    featureSignature = tuple(
        (cat, tuple(sorted(featuresByCategory[cat])))
        for cat in catSequence
    )

    return (destCategory, isLemma, featureSignature)
```

**Pros:**
- Semantic matching based on function
- No false positives (different features = different keys)
- Clear, understandable logic

**Cons:**
- Complex key construction
- Need to reconstruct feature signature from existing macros (requires parsing macro structure)
- What if user hand-edited macro slightly? Different signature, but functionally equivalent
- Ordering of features might matter or not depending on implementation

**Verdict:** Feasible but complex

##### Alternative B: Macro Structure Fingerprinting (CHOSEN)

**Description:** Hash the normalized macro structure
```python
def GetMacroFingerprint(macro: ET.Element) -> str:
    """Generate hash of macro structure for deduplication."""
    # Serialize macro content
    content = ET.tostring(macro, encoding='unicode')

    # Normalize variable names and positions
    content = re.sub(r'n="[^"]*"', 'n="VAR"', content)
    content = re.sub(r'pos="[^"]*"', 'pos="POS"', content)

    # Hash normalized content
    import hashlib
    return hashlib.md5(content.encode()).hexdigest()
```

**Pros:**
- Detects truly identical logic regardless of naming
- No false positives from naming differences
- Works with hand-edited macros (different structure = different hash)
- Simple to implement and understand
- Easy to debug (can inspect normalized structure)

**Cons:**
- Doesn't catch "semantically equivalent but structurally different" macros
- Hash collisions theoretically possible (but MD5 is fine for this use)
- Sensitive to whitespace/formatting changes

**Verdict:** Chosen - Best balance of simplicity and correctness

##### Alternative C: AST-Based Semantic Comparison

**Description:** Parse macro into abstract syntax tree, compare logic
```python
def compareMacroSemantics(macro1, macro2):
    """Compare macros at semantic level."""
    ast1 = parseMacroToAST(macro1)
    ast2 = parseMacroToAST(macro2)
    return areEquivalent(ast1, ast2)
```

**Pros:**
- Could detect semantically equivalent macros with different structure
- Most sophisticated approach

**Cons:**
- Very complex to implement
- Need to define semantic equivalence rules
- Slow (parsing, comparison)
- Overkill for the problem
- Risk of false positives if equivalence rules wrong

**Verdict:** Rejected - Unnecessary complexity

##### Alternative D: Disable Macro Reuse Permanently

**Description:** Keep macro reuse disabled, accept duplication

**Pros:**
- No implementation effort
- No risk of bugs

**Cons:**
- 30-50% code duplication
- Larger files
- Slower processing
- More memory usage
- Doesn't address user needs

**Verdict:** Rejected - Unacceptable long-term

#### Rationale

**Why we chose fingerprinting:**

1. **Correct deduplication:** Detects truly identical macros while avoiding false positives

2. **Simple implementation:** Hash-based approach is straightforward to code and test

3. **Robust to hand-editing:** User-modified macros get different hashes, won't be incorrectly reused

4. **Debuggable:** Can inspect normalized structure and hash to understand matches/mismatches

5. **Good enough:** Perfect deduplication (catching all semantic equivalences) is not necessary. Catching identical structures is sufficient and valuable.

6. **Proven technique:** Fingerprinting/hashing is a well-established deduplication method

#### Trade-offs and Implications

**Benefits:**
- ✓ 30-50% reduction in redundant macros
- ✓ Smaller transfer files
- ✓ Faster processing
- ✓ Less memory usage
- ✓ Simple, maintainable code

**Costs:**
- ✗ Won't catch all semantically equivalent macros (only identical structure)
- ✗ Whitespace/formatting changes could affect hash (mitigated by normalization)
- ✗ Need to compute hash for every macro (negligible performance cost)

**Risks:**
- ✓ Low: Hash collisions extremely unlikely with MD5 for this purpose
- ✓ Low: Normalization logic might have edge cases (mitigated by testing)

**Implementation Impact:**
- New method: `GetMacroFingerprint()`
- Modified method: `GetMultiFeatureMacro()` to check fingerprints
- Modified method: `ProcessExistingTransferFile()` to compute fingerprints
- New data structure: `self.macroFingerprints` dict
- Test coverage: Comprehensive tests for deduplication scenarios

#### Implementation Notes

**Key implementation:**

```python
class RuleGenerator:
    def __init__(self, ...):
        self.macroFingerprints = {}  # fingerprint → MacroSpec

    def GetMacroFingerprint(self, macro: ET.Element) -> str:
        """Generate fingerprint hash of macro structure."""
        content = ET.tostring(macro, encoding='unicode')

        # Normalize: Replace variable names and positions
        content = re.sub(r'n="[^"]*"', 'n="VAR"', content)
        content = re.sub(r'pos="[^"]*"', 'pos="POS"', content)

        # Normalize whitespace
        content = re.sub(r'\s+', ' ', content)

        # Hash
        import hashlib
        return hashlib.md5(content.encode()).hexdigest()

    def GetMultiFeatureMacro(self, destCategory, isLemma, sources):
        """Get or create macro with fingerprint deduplication."""

        # Generate macro (temporary)
        macro = self._GenerateMacro(destCategory, isLemma, sources)

        # Check if equivalent macro exists
        fingerprint = self.GetMacroFingerprint(macro)

        if fingerprint in self.macroFingerprints:
            # Reuse existing
            spec = self.macroFingerprints[fingerprint]
            self.report.Info(f"Reusing macro {spec.macid}")
            return spec
        else:
            # Add new
            macroId = self.GetAvailableID(...)
            macro.set('n', macroId)

            # Store fingerprint
            spec = MacroSpec(macid=macroId, ...)
            self.macroFingerprints[fingerprint] = spec

            # Add to transfer file
            self.GetSection('section-def-macros').append(macro)

            return spec
```

**Testing strategy:**
- Test identical macros → same fingerprint
- Test different macros → different fingerprints
- Test renamed variables → same fingerprint (normalized)
- Test renamed macros → same fingerprint (normalized)
- Test different logic → different fingerprints
- Test hand-edited macros → different fingerprints
- Performance test: Large projects with many macros

#### Review History

- 2025-11-22: Decision proposed based on Issue #661 analysis
- 2025-11-22: Decision accepted
- Status: Ready for implementation in Phase 3

---

### DEC-003: DTD Schema Extension for Infix and Circumfix

**Status:** Accepted
**Date:** 2025-11-22
**Deciders:** Development Team
**Priority:** High
**Related Issues:** Implementation Plan Section 2

#### Context and Problem

The current DTD only supports prefix and suffix affixes:
```xml
<!ATTLIST Affix
  type (prefix | suffix) "suffix"
>
```

However:
1. FLEx already recognizes infix and circumfix types (GUIDs defined in Utils.py)
2. Many languages use infixes (Tagalog, Austronesian languages) and circumfixes (German, Indonesian)
3. Rule Assistant cannot handle these languages properly
4. No way to specify infix position or circumfix splitting

#### Decision

**Extend the DTD to support all four affix types with position specification.**

New DTD:
```xml
<!ATTLIST Affix
  type (prefix | suffix | infix | circumfix) "suffix"
  position CDATA #IMPLIED
  comment CDATA #IMPLIED
>
```

#### Alternatives Considered

##### Alternative A: Minimal Extension (Types Only)

**Description:** Add infix/circumfix types but no position attribute
```xml
<!ATTLIST Affix
  type (prefix | suffix | infix | circumfix) "suffix"
>
```

**Pros:**
- Minimal change
- Backward compatible (default="suffix")
- Recognizes the types

**Cons:**
- No way to specify where infix goes
- Infixes can't be handled properly
- Would need to add position later anyway

**Verdict:** Insufficient

##### Alternative B: Full Extension with Position (CHOSEN)

**Description:** Add types and position attribute
```xml
<!ATTLIST Affix
  type (prefix | suffix | infix | circumfix) "suffix"
  position CDATA #IMPLIED
  comment CDATA #IMPLIED
>
```

**Pros:**
- Complete solution
- Supports infix position specification
- Supports comments for documentation
- Extensible for future needs
- Backward compatible (position is optional)

**Cons:**
- More complex
- Need to define position specification format

**Verdict:** Chosen - Complete solution

##### Alternative C: Separate Elements for Each Type

**Description:** Different XML elements for different affix types
```xml
<!ELEMENT Prefix (Features*) >
<!ELEMENT Suffix (Features*) >
<!ELEMENT Infix (Features*) >
  <!ATTLIST Infix position CDATA #REQUIRED>
<!ELEMENT Circumfix (Features*) >
```

**Pros:**
- Very explicit
- Position required for infixes (type-safe)

**Cons:**
- Not backward compatible
- Breaks existing files
- More complex DTD
- Harder to process programmatically

**Verdict:** Rejected - Breaks backward compatibility

##### Alternative D: Defer to Future Version

**Description:** Don't add infix/circumfix support now, wait for major version

**Pros:**
- No immediate work
- Can design more carefully

**Cons:**
- Doesn't address user needs
- Languages with infixes/circumfixes unsupported
- Arbitrary delay

**Verdict:** Rejected - Need this functionality now

#### Rationale

**Why we chose full extension with position:**

1. **Complete functionality:** Supports all four affix types properly

2. **Backward compatible:**
   - Default type is "suffix" (existing behavior)
   - Position is optional (#IMPLIED)
   - Existing files work unchanged

3. **Future-proof:** Position and comment attributes allow for:
   - Infix position specification
   - Documentation
   - Future enhancements

4. **Matches FLEx:** FLEx already has these affix types, we're just exposing them

5. **User need:** Real languages need this, not a theoretical feature

#### Trade-offs and Implications

**Benefits:**
- ✓ Support for Austronesian languages (infixes)
- ✓ Support for German, Indonesian, Berber (circumfixes)
- ✓ Backward compatible
- ✓ Extensible

**Costs:**
- ✗ DTD complexity increases
- ✗ Need to update two DTD files (main + XMLmind)
- ✗ Need to update processing code
- ✗ Need to document position format

**Risks:**
- ✓ Low: Changes are additive, don't break existing files
- ⚠️ Medium: Infix positioning is complex (Apertium limitations)

**Implementation Impact:**
- DTD file updates (2 files)
- CreateApertiumRules.py: Add infix/circumfix processing
- Documentation: Position format specification
- Examples: Create sample infix and circumfix rules

**Apertium Limitations:**
- Apertium transfer rules don't natively support infix positioning
- Infixes will be treated as prefixes with a warning comment
- Future: Could integrate with Apertium morphological analyzer for proper handling

#### Implementation Notes

**Position Specification Format:**

For infixes, the `position` attribute can be:
- `"after-first-consonant"` - Insert after first consonant
- `"before-last-vowel"` - Insert before final vowel
- `"after-first-vowel"` - Insert after first vowel
- `"character:N"` - Insert after character N (0-indexed)
- `"regex:PATTERN"` - Insert at position matching regex

For circumfixes, position is not used (wraps entire stem).

**Example Usage:**

Tagalog infix -um-:
```xml
<Affix type="infix" position="after-first-consonant">
  <Features>
    <Feature label="tense" value="past"/>
  </Features>
</Affix>
```

German circumfix ge-...-t:
```xml
<Affix type="circumfix">
  <Features>
    <Feature label="aspect" value="participle"/>
  </Features>
</Affix>
```

**Generated Apertium:**

Infixes will generate warning comments:
```xml
<!--WARNING: Infix positioning (after-first-consonant) not fully supported in Apertium transfer.
    Treating as prefix. Consider using Apertium morphological analyzer for proper infix handling.-->
```

Circumfixes will split into prefix and suffix parts:
```xml
<!--Circumfix: prefix and suffix parts generated from same feature-->
<lit-tag v="GE"/><!-- circumfix prefix -->
<!-- stem -->
<lit-tag v="T"/><!-- circumfix suffix -->
```

**Testing:**
- Unit tests for DTD validation
- Integration tests for infix processing
- Integration tests for circumfix processing
- End-to-end tests with sample languages

#### Review History

- 2025-11-22: Decision proposed
- 2025-11-22: Decision accepted
- Status: Ready for implementation in Phase 4

---

### DEC-004: Data Source of Truth and Synchronization

**Status:** Accepted
**Date:** 2025-11-22
**Deciders:** Development Team, User Feedback
**Priority:** Critical
**Related Issues:** FR-002, DOC-001

#### Context and Problem

Users are confused about:
1. Where Rule Assistant gets its data
2. Whether manual .t1x edits are preserved
3. Whether FLEx changes are reflected
4. When to reload what

This confusion leads to:
- Fear of data loss
- Reluctance to use RA
- Inability to combine RA + manual editing
- Support burden

**Root cause:** No clear architectural principle for data sources and synchronization.

#### Decision

**Establish clear data source hierarchy:**

1. **FLEx Database = Single Source of Truth for Morphological Data**
   - Categories, features, affixes, stems
   - Always queried fresh (startup, reload, generation)
   - Never cached across sessions

2. **Rule Assistant XML = Single Source of Truth for Rule Structure**
   - Pattern specifications
   - Feature mapping
   - User's rule definitions

3. **Transfer .t1x = Generated Output (May Contain Manual Edits)**
   - Generated from #1 + #2
   - May have user's manual additions
   - All sections preserved
   - Merged, not overwritten

#### Alternatives Considered

##### Alternative A: Cache-Based Approach

**Description:** Cache FLEx data for performance
```
Startup:
  - Query FLEx database
  - Cache all data
  - Use cache for all operations
  - Only refresh when user explicitly reloads
```

**Pros:**
- Faster (no repeated queries)
- Consistent data during session

**Cons:**
- Stale data if FLEx changes
- Cache invalidation is hard
- More complex state management
- Doesn't match user mental model

**Verdict:** Rejected - Complexity not worth it

##### Alternative B: .t1x as Source of Truth

**Description:** Treat .t1x as authoritative, RA just modifies it
```
Startup:
  - Load .t1x file
  - Extract categories, attributes from it
  - Don't query FLEx
  - Just add rules
```

**Pros:**
- Simple file-based workflow
- Fast (no database queries)

**Cons:**
- FLEx changes not reflected
- .t1x gets out of sync with FLEx
- No way to use current FLEx data
- Defeats purpose of FLEx integration

**Verdict:** Rejected - Doesn't leverage FLEx

##### Alternative C: Explicit Synchronization Points (CHOSEN)

**Description:** Clear, explicit data flow
```
Startup:
  - Query FLEx database (fresh data)
  - Load Rule Assistant XML (rule specs)
  - Display what data is loaded

Generation:
  - Query FLEx for current data
  - Load existing .t1x to preserve
  - Merge and write

Reload (new feature):
  - Re-query FLEx database
  - Update GUI
  - Warn about conflicts
```

**Pros:**
- Clear, understandable
- Always uses current FLEx data
- Preserves user edits
- Explicit control

**Cons:**
- More database queries (small cost)
- Need "Reload" button

**Verdict:** Chosen - Best matches user mental model

#### Rationale

**Why we chose explicit synchronization:**

1. **Matches user expectations:** "When I change FLEx, RA should see it"

2. **Prevents stale data:** Always using current FLEx data prevents errors

3. **Clear mental model:** Users understand where data comes from

4. **Enables mixed workflow:** Can use both RA and manual editing safely

5. **Simple to explain:** Documentation can clearly state data sources

6. **Debugging is easier:** When problems occur, clear where to look

#### Trade-offs and Implications

**Benefits:**
- ✓ No stale data issues
- ✓ FLEx changes always reflected
- ✓ Clear, understandable behavior
- ✓ Users trust the system

**Costs:**
- ✗ More database queries (small performance impact)
- ✗ Need "Reload FLEx Data" button (future work)

**Risks:**
- ✓ Low: FLEx queries are fast enough
- ✓ Low: Clear documentation reduces confusion

**Implementation Impact:**
- Status bar showing data sources and timestamps
- "Reload FLEx Data" button (future)
- Clear documentation in user guide
- Warning dialogs before overwriting

#### Implementation Notes

**Data Flow Documentation:**

```
┌─────────────┐
│   FLEx DB   │ ← Single Source of Truth for Morphological Data
└──────┬──────┘
       │ Query on: startup, reload, generation
       ▼
┌─────────────┐
│   RA XML    │ ← Single Source of Truth for Rule Structure
└──────┬──────┘
       │ Read on: generation
       ▼
┌─────────────┐
│  .t1x file  │ ← Generated Output (May Have Manual Edits)
└─────────────┘   All sections preserved, merged not overwritten
```

**Status Bar Information:**

```
✓ FLEx Data: Loaded 2025-11-22 14:32 (English-Spanish)
✓ Transfer File: transfer_rules.t1x (modified 14:15)
✓ Contains: 5 rules, 12 macros, 3 variables
```

**"Reload FLEx Data" Button (Future):**

```python
def ReloadFLExData(self):
    """Reload morphological data from FLEx database."""
    # Query FLEx
    categories = db.getAllCategories()
    features = db.getAllFeatures()
    affixes = db.getAllAffixes()

    # Update GUI
    self.updateCategoryDropdowns(categories)
    self.updateFeatureDropdowns(features)

    # Check for conflicts
    conflicts = self.checkForConflicts()
    if conflicts:
        self.showWarning(f"Warning: {len(conflicts)} rules reference deleted features")

    # Display summary
    self.showInfo(f"Reloaded: {len(categories)} categories, {len(features)} features")
```

**Warning Dialogs:**

Before overwriting existing .t1x:
```
Warning: Transfer file already exists

File: C:\MyProject\transfer_rules.t1x
Last modified: 2025-11-22 14:15
Contains: 5 rules, 12 macros, 3 variables

If you continue:
✓ All existing sections will be preserved
✓ New rules will be added
⚠️ Rules with same names may be duplicated (check overwrite_rules setting)

Backup recommended before continuing.

[Cancel] [Continue]
```

#### Review History

- 2025-11-22: Decision proposed
- 2025-11-22: Decision accepted
- Status: Partial implementation (need reload button)

---

### DEC-005: Proper Noun Capitalization Handling

**Status:** Accepted
**Date:** 2025-11-22
**Deciders:** Development Team
**Priority:** Medium
**Related Issues:** Implementation Plan TODO Line 1252

#### Context and Problem

Current code applies capitalization based solely on word position:
```python
# Line 1252
# TODO: check that it's not a proper noun
if index == 0:
    # Capitalize first word
    lemCase = ET.SubElement(lu, 'get-case-from', pos='1')
```

Problem: Proper nouns should maintain capitalization regardless of position:
- "John" should stay "John" even in non-initial position
- Common nouns should follow position rules

#### Decision

**Skip case handling for proper nouns; preserve their original capitalization.**

Implementation:
```python
isProperNoun = self.IsProperNoun(cat, pos)

if not isProperNoun:
    # Apply position-based capitalization for common nouns
    if index == 0:
        lemCase = ET.SubElement(lu, 'get-case-from', pos='1')
else:
    # Preserve original capitalization for proper nouns
    lemCase = lu
```

#### Alternatives Considered

##### Alternative A: Skip Case Handling (CHOSEN)

**Description:** Don't add `<get-case-from>` for proper nouns

**Pros:**
- Simple
- Preserves proper noun capitalization
- Matches linguistic expectations

**Cons:**
- Need to detect proper nouns

**Verdict:** Chosen

##### Alternative B: Use Apertium's Proper Noun Tag

**Description:** Tag proper nouns with `<np>`, let Apertium handle them
```xml
<lit-tag v="np"/>
```

**Pros:**
- Apertium-native solution
- Standard approach

**Cons:**
- Requires modifying category tags
- May conflict with existing tagging
- Need to ensure Apertium recognizes the tag

**Verdict:** Possible future enhancement

##### Alternative C: Always Capitalize Proper Nouns

**Description:** Force capitalization of proper nouns
```python
if isProperNoun:
    # Force capitalize
    ET.SubElement(lu, 'get-case-from', pos='0')
```

**Pros:**
- Explicit

**Cons:**
- Wrong if proper noun in all caps
- Wrong if proper noun has internal capitals (McDonald)
- Better to preserve original

**Verdict:** Rejected

#### Rationale

**Why skip case handling for proper nouns:**

1. **Linguistic correctness:** Proper nouns maintain their capitalization

2. **User expectations:** "John" should not become "john"

3. **Minimal change:** Skip existing logic, don't add complex handling

4. **Apertium compatible:** Letting Apertium preserve original case works

#### Trade-offs and Implications

**Benefits:**
- ✓ Linguistically correct output
- ✓ Simple implementation

**Costs:**
- ✗ Need proper noun detection logic

**Risks:**
- ⚠️ Proper noun detection might have false positives/negatives
- Mitigated: Can tune detection logic based on testing

**Implementation Impact:**
- New method: `IsProperNoun(cat, pos)`
- Modified: Capitalization logic in `ProcessRule()`
- Test coverage: Proper nouns in various positions

#### Implementation Notes

**Proper Noun Detection:**

```python
def IsProperNoun(self, category: str, pos: Optional[str] = None) -> bool:
    """Check if a category represents a proper noun."""
    # Check for common proper noun markers
    properNounMarkers = ['np', 'prop', 'proper', 'propn', 'PN']

    catLower = category.lower()
    if any(marker in catLower for marker in properNounMarkers):
        return True

    # Check category hierarchy
    hierarchy = Utils.getCategoryHierarchy(self.targetDB, category)
    return any(marker in cat.lower()
              for cat in hierarchy
              for marker in properNounMarkers)
```

**Usage:**

```python
# In ProcessRule()
isProperNoun = self.IsProperNoun(cat, pos if pos else None)

if not isProperNoun:
    if index == 0 and (pos != '1' or shouldUseLemmaMacro):
        lemCase = ET.SubElement(lu, 'get-case-from', pos='1')
    elif index > 0 and pos == '1' and index < len(sourceWords):
        lemCase = ET.SubElement(lu, 'get-case-from', pos=str(index+1))
    else:
        lemCase = lu
else:
    lemCase = lu  # No case modification for proper nouns
```

**Testing:**
- Proper noun in initial position (should stay capitalized)
- Proper noun in non-initial position (should stay capitalized)
- Common noun in initial position (should be capitalized)
- Common noun in non-initial position (should be lowercase)

#### Review History

- 2025-11-22: Decision proposed
- 2025-11-22: Decision accepted
- Status: Ready for implementation in Phase 2

---

### DEC-006: Phase-Based Implementation Strategy

**Status:** Accepted
**Date:** 2025-11-22
**Deciders:** Development Team, Project Management
**Priority:** High
**Related Issues:** Implementation Plan, Roadmap

#### Context and Problem

We have identified:
- 4 TODO items to resolve
- 2 critical bugs (FR-001, FR-002)
- 1 disabled feature to re-enable (Issue #661)
- 2 new affix types to add (infix, circumfix)
- Multiple UX improvements
- Documentation needs

**Question:** What order should we tackle these in?

#### Decision

**Implement in 5 phases based on priority and dependencies:**

1. **Phase 1: Critical Bug Fixes (2 weeks)**
   - FR-001: Section preservation
   - FR-002: Data synchronization documentation
   - FR-003: Error message improvements

2. **Phase 2: Technical Debt (2 weeks)**
   - TODO items (4 total)
   - Code cleanup
   - Test coverage

3. **Phase 3: Macro Reuse (2-3 weeks)**
   - Issue #661 fix
   - Fingerprinting implementation
   - Testing and validation

4. **Phase 4: Affix Types (3 weeks)**
   - DTD updates
   - Infix support
   - Circumfix support
   - Examples and testing

5. **Phase 5: UX Improvements (2-3 weeks)**
   - "Reload FLEx Data" button
   - Template management
   - Documentation completion

#### Alternatives Considered

##### Alternative A: Feature-Based Approach

**Description:** Group by feature area
- All affix work together
- All macro work together
- All UX work together

**Pros:**
- Logical grouping

**Cons:**
- Critical bugs delayed
- Users wait longer for fixes
- Risk not prioritized

**Verdict:** Rejected

##### Alternative B: Priority-Only Approach

**Description:** Do highest priority items first, regardless of category

**Pros:**
- High-value work first

**Cons:**
- May create dependencies issues
- Jumping between areas
- Less efficient

**Verdict:** Partial (Phase 1)

##### Alternative C: Phased Approach (CHOSEN)

**Description:** Phases based on priority + logical grouping

**Pros:**
- Critical fixes first
- Logical progression
- Clear milestones
- Manages dependencies

**Cons:**
- Some features delayed

**Verdict:** Chosen

#### Rationale

**Why phased approach:**

1. **Critical issues first:** Data loss bugs must be fixed immediately

2. **Foundation before features:** Clean up technical debt before adding complexity

3. **Dependencies managed:** Each phase builds on previous

4. **Clear milestones:** Easy to track progress, communicate status

5. **Risk reduction:** Tackle risky items (macro reuse) with solid foundation

6. **User value delivery:** Users get benefits incrementally

#### Trade-offs and Implications

**Benefits:**
- ✓ Clear roadmap
- ✓ Critical issues addressed first
- ✓ Incremental value delivery
- ✓ Manageable chunks

**Costs:**
- ✗ Some features delayed
- ✗ Need coordination between phases

**Risks:**
- ⚠️ Phase overruns could delay later phases
- Mitigated: Buffer time between phases

**Timeline:**
- Phase 1: Weeks 1-2
- Phase 2: Weeks 3-4
- Phase 3: Weeks 5-7
- Phase 4: Weeks 8-10
- Phase 5: Weeks 11-13
- Total: ~13 weeks (3 months)

#### Implementation Notes

**Phase Gating:**

Each phase has:
- Clear deliverables
- Acceptance criteria
- Review checkpoint
- Go/no-go decision for next phase

**Phase 1 Deliverables:**
- [ ] FR-001 fixed: Sections preserved
- [ ] FR-002 documented: Data flow clear
- [ ] FR-003 fixed: Error messages improved
- [ ] All regression tests pass
- [ ] User guide updated

**Phase 1 Acceptance:**
- No data loss issues
- Users understand data flow
- Error messages helpful

**Proceed to Phase 2:** YES/NO

#### Review History

- 2025-11-22: Decision proposed
- 2025-11-22: Decision accepted
- Status: Approved for implementation

---

## Superseded Decisions

(None currently)

---

## Deferred Decisions

### DEF-001: Advanced Phonological Rule Integration

**Status:** Deferred
**Date:** 2025-11-22
**Reason:** Complexity too high for current scope; requires STAMP integration
**Future Timeline:** Phase 4 (6-12 months)

**Context:**
Morpheme combinations often trigger phonological changes. Full support would require:
- Integration with FLExTrans phonological rule system
- Rule triggering based on morpheme boundaries
- Ordering control for rule application

**Deferred because:**
- Complex integration with STAMP
- Requires architectural work beyond current scope
- Few users need this immediately
- Can be added later without breaking changes

**Reconsider when:**
- Phases 1-5 complete
- User demand increases
- STAMP integration architecture designed

---

### DEF-002: Reduplication Support

**Status:** Deferred
**Date:** 2025-11-22
**Reason:** Very complex; Apertium limitations; low immediate demand
**Future Timeline:** Phase 4+ (12+ months)

**Context:**
Reduplication (full or partial word repetition) is used in many languages but very complex to implement:
- Full reduplication: *rumah-rumah* "houses" (Indonesian)
- Partial reduplication: *s-um-ulat* "wrote" (Tagalog)

**Deferred because:**
- Apertium doesn't natively support reduplication
- Would require morphological analyzer integration
- Very few users need this immediately
- Infixes (similar complexity) higher priority

**Reconsider when:**
- Infix/circumfix support complete
- Apertium integration architecture proven
- User demand for specific language pair

---

### DEF-003: Templatic Morphology (Non-Concatenative)

**Status:** Deferred
**Date:** 2025-11-22
**Reason:** Fundamental architectural change required; niche use case
**Future Timeline:** Phase 4+ (18+ months)

**Context:**
Semitic languages use root-and-pattern morphology:
- Root: K-T-B (concept: "writing")
- Pattern: CaCaCa → *kataba* "he wrote"
- Pattern: maCCūC → *maktūb* "written"

**Deferred because:**
- Apertium assumes concatenative morphology
- Would require major architectural changes
- Very specialized use case (Semitic, some Cushitic)
- May need STAMP integration or Apertium extension

**Reconsider when:**
- All concatenative morphology work complete
- Partnership with Semitic language project
- STAMP or Apertium provides support

---

## Appendix: Decision-Making Process

### When to Create a Decision

Create a decision record when:
- ✓ The decision affects architecture significantly
- ✓ Multiple alternatives exist
- ✓ The decision has long-term implications
- ✓ The decision involves trade-offs
- ✓ The decision might be questioned later

Don't create a decision record for:
- ✗ Obvious implementation details
- ✗ Coding style choices
- ✗ Minor refactoring
- ✗ Bug fixes (unless architectural)

### Decision Review Process

1. **Proposal:** Team member drafts decision using template
2. **Review:** Team discusses alternatives and trade-offs
3. **Acceptance:** Team reaches consensus, decision marked "Accepted"
4. **Implementation:** Decision implemented
5. **Retrospective:** After implementation, review if decision was correct

### Changing Decisions

To change an accepted decision:
1. Create new decision that supersedes the old one
2. Mark old decision as "Superseded"
3. Link new decision to old one
4. Document why change was needed

### Decision Ownership

**Who decides:**
- Architecture decisions: Development team lead + team
- UX decisions: UI/UX developer + user feedback
- Priority decisions: Project manager + stakeholders
- Technical decisions: Development team

---

**Document Maintenance:**
- Review quarterly
- Update as decisions are made
- Archive superseded decisions
- Keep deferred decisions list current

---

*This decision log synthesizes analysis from the Implementation Plan, Roadmap, Feature Requests, and Test Coverage documents to provide comprehensive decision documentation for the Rule Assistant enhancement project.*
